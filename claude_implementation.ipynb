{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "import numpy as np\n",
    "from typing import Tuple, List, Optional\n",
    "\n",
    "class InceptionBlock(nn.Module):\n",
    "    \"\"\"Implementation of Inception block used in FaceNet\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        ch1x1: int,\n",
    "        ch3x3red: int,\n",
    "        ch3x3: int,\n",
    "        ch5x5red: int,\n",
    "        ch5x5: int,\n",
    "        pool_proj: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1x1 conv branch\n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, ch1x1, kernel_size=1),\n",
    "            nn.BatchNorm2d(ch1x1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # 1x1 conv -> 3x3 conv branch\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, ch3x3red, kernel_size=1),\n",
    "            nn.BatchNorm2d(ch3x3red),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(ch3x3red, ch3x3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(ch3x3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # 1x1 conv -> 5x5 conv branch (implemented as two 3x3 convs)\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, ch5x5red, kernel_size=1),\n",
    "            nn.BatchNorm2d(ch5x5red),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(ch5x5red, ch5x5, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(ch5x5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(ch5x5, ch5x5, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(ch5x5),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Max pool -> 1x1 conv branch\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_channels, pool_proj, kernel_size=1),\n",
    "            nn.BatchNorm2d(pool_proj),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "        branch3 = self.branch3(x)\n",
    "        branch4 = self.branch4(x)\n",
    "        \n",
    "        # Concatenate along channel dimension\n",
    "        return mx.concatenate([branch1, branch2, branch3, branch4], axis=1)\n",
    "\n",
    "class FaceNet(nn.Module):\n",
    "    \"\"\"FaceNet implementation in MLX\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_size: int = 128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initial convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Inception blocks\n",
    "        self.inception1 = InceptionBlock(64, 64, 96, 128, 16, 32, 32)\n",
    "        self.inception2 = InceptionBlock(256, 128, 128, 192, 32, 96, 64)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.inception3a = InceptionBlock(480, 192, 96, 208, 16, 48, 64)\n",
    "        self.inception3b = InceptionBlock(512, 160, 112, 224, 24, 64, 64)\n",
    "        self.inception3c = InceptionBlock(512, 128, 128, 256, 24, 64, 64)\n",
    "        self.inception4a = InceptionBlock(512, 112, 144, 288, 32, 64, 64)\n",
    "        self.inception4b = InceptionBlock(528, 256, 160, 320, 32, 128, 128)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.inception4c = InceptionBlock(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.inception4d = InceptionBlock(832, 384, 192, 384, 48, 128, 128)\n",
    "        \n",
    "        # Final layers\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=7, stride=1)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc = nn.Linear(1024, embedding_size)\n",
    "        self.bn2 = nn.BatchNorm1d(embedding_size)\n",
    "        \n",
    "        # L2 normalization\n",
    "        self.l2_norm = lambda x: x / mx.sqrt(mx.sum(mx.square(x), axis=1, keepdims=True))\n",
    "    \n",
    "    def __call__(self, x, training: bool = False):\n",
    "        # Initial convolutional layers\n",
    "        x = self.maxpool1(self.relu(self.bn1(self.conv1(x))))\n",
    "        \n",
    "        # Inception blocks\n",
    "        x = self.inception1(x)\n",
    "        x = self.inception2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        x = self.inception3a(x)\n",
    "        x = self.inception3b(x)\n",
    "        x = self.inception3c(x)\n",
    "        x = self.inception4a(x)\n",
    "        x = self.inception4b(x)\n",
    "        x = self.maxpool3(x)\n",
    "        \n",
    "        x = self.inception4c(x)\n",
    "        x = self.inception4d(x)\n",
    "        \n",
    "        # Final layers\n",
    "        x = self.avgpool(x)\n",
    "        x = mx.reshape(x, (-1, 1024))\n",
    "        if training:\n",
    "            x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        # L2 normalization to create the embedding\n",
    "        embedding = self.l2_norm(x)\n",
    "        return embedding\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"Triplet loss with hard positive/negative mining\"\"\"\n",
    "    \n",
    "    def __init__(self, margin: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def __call__(self, embeddings, labels):\n",
    "        # Get the pairwise distance matrix\n",
    "        pairwise_dist = self._pairwise_distances(embeddings)\n",
    "        \n",
    "        # For each anchor, get the hardest positive\n",
    "        # First, get a mask for valid positive pairs (same class)\n",
    "        labels = mx.array(labels)\n",
    "        mask_positives = mx.equal(mx.expand_dims(labels, axis=0), mx.expand_dims(labels, axis=1))\n",
    "        \n",
    "        # Exclude the diagonal from mask_positives (distance to self is 0, not a useful positive)\n",
    "        mask_positives = mask_positives & (1 - mx.eye(labels.shape[0], dtype=mx.bool_))\n",
    "        \n",
    "        # Get hardest positives (maximum distance to positive samples)\n",
    "        hardest_positive_dist = mx.max(pairwise_dist * mask_positives.astype(mx.float32), axis=1)\n",
    "        \n",
    "        # For each anchor, get the hardest negative\n",
    "        # First, get a mask for valid negative pairs (different class)\n",
    "        mask_negatives = ~mask_positives\n",
    "        \n",
    "        # Make invalid negatives have large distance so they won't be selected\n",
    "        max_dist = mx.max(pairwise_dist)\n",
    "        neg_dist = pairwise_dist * mask_negatives.astype(mx.float32) + max_dist * (~mask_negatives).astype(mx.float32)\n",
    "        \n",
    "        # Get hardest negatives (minimum distance to negative samples)\n",
    "        hardest_negative_dist = mx.min(neg_dist, axis=1)\n",
    "        \n",
    "        # Calculate triplet loss\n",
    "        triplet_loss = mx.maximum(hardest_positive_dist - hardest_negative_dist + self.margin, 0.0)\n",
    "        \n",
    "        # Get final mean triplet loss\n",
    "        return mx.mean(triplet_loss)\n",
    "    \n",
    "    def _pairwise_distances(self, embeddings):\n",
    "        \"\"\"Compute the 2D matrix of distances between all embeddings.\"\"\"\n",
    "        # Get dot product (batch_size, batch_size)\n",
    "        dot_product = mx.matmul(embeddings, mx.transpose(embeddings))\n",
    "        \n",
    "        # Get squared L2 norm for each embedding (batch_size, 1)\n",
    "        square_norm = mx.sum(mx.square(embeddings), axis=1)\n",
    "        \n",
    "        # Calculate pairwise distance matrix \n",
    "        # ||a - b||^2 = ||a||^2 + ||b||^2 - 2 * a.dot(b)\n",
    "        distances = mx.expand_dims(square_norm, 1) + mx.expand_dims(square_norm, 0) - 2.0 * dot_product\n",
    "        \n",
    "        # Because of computation errors, some distances might be negative\n",
    "        distances = mx.maximum(distances, 0.0)\n",
    "        \n",
    "        # If we want the actual distance, we can use sqrt\n",
    "        return mx.sqrt(distances)\n",
    "\n",
    "# Data preprocessing function\n",
    "def preprocess_image(image_path: str, target_size: Tuple[int, int] = (160, 160)) -> mx.array:\n",
    "    \"\"\"\n",
    "    Load and preprocess an image for FaceNet.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image file\n",
    "        target_size: Target size for the image (height, width)\n",
    "        \n",
    "    Returns:\n",
    "        Preprocessed image as MLX array\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import cv2\n",
    "        # Load image\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Unable to load image from {image_path}\")\n",
    "        \n",
    "        # Convert BGR to RGB\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Resize to target size\n",
    "        img = cv2.resize(img, target_size[::-1])  # CV2 takes (width, height)\n",
    "        \n",
    "        # Normalize pixel values to [-1, 1]\n",
    "        img = img.astype(np.float32) / 127.5 - 1.0\n",
    "        \n",
    "        # Convert to MLX array and add batch dimension\n",
    "        return mx.array(img).transpose(2, 0, 1)  # Convert to channels-first format\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error preprocessing image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example of loading pretrained weights from a PyTorch model\n",
    "def load_pretrained_weights(facenet_mlx: FaceNet, torch_model_path: str) -> FaceNet:\n",
    "    \"\"\"\n",
    "    Load pretrained weights from a PyTorch FaceNet model.\n",
    "    \n",
    "    Args:\n",
    "        facenet_mlx: MLX FaceNet model\n",
    "        torch_model_path: Path to PyTorch model file\n",
    "        \n",
    "    Returns:\n",
    "        MLX FaceNet model with loaded weights\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import torch\n",
    "        \n",
    "        # Load PyTorch model\n",
    "        torch_model = torch.load(torch_model_path, map_location=\"cpu\")\n",
    "        \n",
    "        # Create a state dict for MLX model\n",
    "        state_dict = {}\n",
    "        \n",
    "        # Map PyTorch parameter names to MLX parameter names\n",
    "        # This is a simplified example - you would need to adapt based on your specific models\n",
    "        for name, param in torch_model.items():\n",
    "            # Convert PyTorch tensor to numpy array\n",
    "            param_np = param.numpy()\n",
    "            \n",
    "            # Handle convolution weight format differences (PyTorch uses OIHW, MLX uses OIHW)\n",
    "            if len(param_np.shape) == 4:\n",
    "                # No need to transpose for this case, format is the same\n",
    "                pass\n",
    "            \n",
    "            # Add to MLX state dict - you might need to adjust the naming convention here\n",
    "            # based on your specific PyTorch model\n",
    "            state_dict[name] = mx.array(param_np)\n",
    "        \n",
    "        # Load state dict into MLX model\n",
    "        facenet_mlx.load_weights(state_dict)\n",
    "        \n",
    "        print(\"Loaded pretrained weights from PyTorch model\")\n",
    "        return facenet_mlx\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading pretrained weights: {e}\")\n",
    "        return facenet_mlx\n",
    "\n",
    "# Example usage for face recognition\n",
    "def recognize_faces(\n",
    "    model: FaceNet,\n",
    "    unknown_face_path: str,\n",
    "    reference_faces: List[Tuple[str, str]],\n",
    "    threshold: float = 0.7\n",
    ") -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Recognize faces by comparing an unknown face against reference faces.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained FaceNet model\n",
    "        unknown_face_path: Path to unknown face image\n",
    "        reference_faces: List of (person_name, face_image_path) tuples\n",
    "        threshold: Similarity threshold (cosine similarity)\n",
    "        \n",
    "    Returns:\n",
    "        List of (person_name, similarity) tuples for matches above threshold\n",
    "    \"\"\"\n",
    "    # Preprocess unknown face\n",
    "    unknown_face = preprocess_image(unknown_face_path)\n",
    "    if unknown_face is None:\n",
    "        return []\n",
    "    \n",
    "    # Get embedding for unknown face\n",
    "    unknown_embedding = model(mx.expand_dims(unknown_face, 0))[0]\n",
    "    \n",
    "    matches = []\n",
    "    \n",
    "    # Compare with reference faces\n",
    "    for name, face_path in reference_faces:\n",
    "        # Preprocess reference face\n",
    "        ref_face = preprocess_image(face_path)\n",
    "        if ref_face is None:\n",
    "            continue\n",
    "        \n",
    "        # Get embedding for reference face\n",
    "        ref_embedding = model(mx.expand_dims(ref_face, 0))[0]\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity = mx.sum(unknown_embedding * ref_embedding) / (\n",
    "            mx.sqrt(mx.sum(unknown_embedding ** 2)) * mx.sqrt(mx.sum(ref_embedding ** 2))\n",
    "        )\n",
    "        \n",
    "        # Check if similarity is above threshold\n",
    "        similarity_value = similarity.item()\n",
    "        if similarity_value > threshold:\n",
    "            matches.append((name, similarity_value))\n",
    "    \n",
    "    # Sort matches by similarity (highest first)\n",
    "    return sorted(matches, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Training function\n",
    "def train_facenet(\n",
    "    model: FaceNet,\n",
    "    train_data_path: str,\n",
    "    val_data_path: Optional[str] = None,\n",
    "    epochs: int = 10,\n",
    "    batch_size: int = 32,\n",
    "    learning_rate: float = 0.001,\n",
    "    margin: float = 0.2\n",
    "):\n",
    "    \"\"\"\n",
    "    Train FaceNet model using triplet loss.\n",
    "    \n",
    "    Args:\n",
    "        model: FaceNet model\n",
    "        train_data_path: Path to training data directory\n",
    "        val_data_path: Path to validation data directory (optional)\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Batch size\n",
    "        learning_rate: Learning rate\n",
    "        margin: Margin for triplet loss\n",
    "        \n",
    "    Returns:\n",
    "        Trained model\n",
    "    \"\"\"\n",
    "    # Set up optimizer\n",
    "    optimizer = optim.Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    # Set up loss function\n",
    "    triplet_loss = TripletLoss(margin=margin)\n",
    "    \n",
    "    # Define training step\n",
    "    @mx.compile\n",
    "    def train_step(model, images, labels):\n",
    "        def loss_fn(model):\n",
    "            embeddings = model(images, training=True)\n",
    "            loss = triplet_loss(embeddings, labels)\n",
    "            return loss\n",
    "        \n",
    "        loss, grads = nn.value_and_grad(loss_fn)(model)\n",
    "        optimizer.update(model, grads)\n",
    "        return loss\n",
    "    \n",
    "    # TODO: Implement data loading and training loop\n",
    "    # This would require implementing a DataLoader for MLX\n",
    "    # For brevity, only the core training step is shown\n",
    "    \n",
    "    print(\"Training function defined - actual implementation would require a DataLoader\")\n",
    "    return model\n",
    "\n",
    "# Main execution example\n",
    "def main():\n",
    "    # Initialize FaceNet model\n",
    "    facenet = FaceNet(embedding_size=128)\n",
    "    \n",
    "    # If you have pretrained weights\n",
    "    # facenet = load_pretrained_weights(facenet, \"path_to_pytorch_model.pth\")\n",
    "    \n",
    "    # Example: Face recognition\n",
    "    # matches = recognize_faces(\n",
    "    #     model=facenet,\n",
    "    #     unknown_face_path=\"unknown_face.jpg\",\n",
    "    #     reference_faces=[\n",
    "    #         (\"Person1\", \"person1_face.jpg\"),\n",
    "    #         (\"Person2\", \"person2_face.jpg\"),\n",
    "    #     ],\n",
    "    #     threshold=0.7\n",
    "    # )\n",
    "    \n",
    "    # Print matches\n",
    "    # for name, similarity in matches:\n",
    "    #     print(f\"Match: {name}, Similarity: {similarity:.4f}\")\n",
    "    \n",
    "    print(\"FaceNet model successfully initialized\")\n",
    "    return facenet\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlx in /Users/tylergee/anaconda3/envs/mlx/lib/python3.11/site-packages (0.23.2)\n",
      "Requirement already satisfied: opencv-python in /Users/tylergee/anaconda3/envs/mlx/lib/python3.11/site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy in /Users/tylergee/anaconda3/envs/mlx/lib/python3.11/site-packages (2.0.2)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.56.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (101 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/tylergee/anaconda3/envs/mlx/lib/python3.11/site-packages (from matplotlib) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Using cached pillow-11.1.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.1 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/tylergee/anaconda3/envs/mlx/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/tylergee/anaconda3/envs/mlx/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.1-cp311-cp311-macosx_11_0_arm64.whl (8.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.1-cp311-cp311-macosx_11_0_arm64.whl (254 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.56.0-cp311-cp311-macosx_10_9_universal2.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp311-cp311-macosx_11_0_arm64.whl (65 kB)\n",
      "Downloading pillow-11.1.0-cp311-cp311-macosx_11_0_arm64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.56.0 kiwisolver-1.4.8 matplotlib-3.10.1 pillow-11.1.0 pyparsing-3.2.1\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install mlx opencv-python numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "import numpy as np\n",
    "from typing import Tuple, List, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionBlock(nn.Module):\n",
    "    \"\"\"Implementation of Inception block used in FaceNet\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        ch1x1: int,\n",
    "        ch3x3red: int,\n",
    "        ch3x3: int,\n",
    "        ch5x5red: int,\n",
    "        ch5x5: int,\n",
    "        pool_proj: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1x1 conv branch\n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, ch1x1, kernel_size=1),\n",
    "            nn.BatchNorm(ch1x1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # 1x1 conv -> 3x3 conv branch\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, ch3x3red, kernel_size=1),\n",
    "            nn.BatchNorm(ch3x3red),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(ch3x3red, ch3x3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm(ch3x3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # 1x1 conv -> 5x5 conv branch (implemented as two 3x3 convs)\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, ch5x5red, kernel_size=1),\n",
    "            nn.BatchNorm(ch5x5red),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(ch5x5red, ch5x5, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm(ch5x5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(ch5x5, ch5x5, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm(ch5x5),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Max pool -> 1x1 conv branch\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_channels, pool_proj, kernel_size=1),\n",
    "            nn.BatchNorm(pool_proj),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "        branch3 = self.branch3(x)\n",
    "        branch4 = self.branch4(x)\n",
    "        \n",
    "        # Concatenate along channel dimension\n",
    "        return mx.concatenate([branch1, branch2, branch3, branch4], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceNet(nn.Module):\n",
    "    \"\"\"FaceNet implementation in MLX\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_size: int = 128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initial convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm(64)  # Fixed: Changed from BatchNorm2d to BatchNorm\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Inception blocks\n",
    "        self.inception1 = InceptionBlock(64, 64, 96, 128, 16, 32, 32)\n",
    "        self.inception2 = InceptionBlock(256, 128, 128, 192, 32, 96, 64)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.inception3a = InceptionBlock(480, 192, 96, 208, 16, 48, 64)\n",
    "        self.inception3b = InceptionBlock(512, 160, 112, 224, 24, 64, 64)\n",
    "        self.inception3c = InceptionBlock(512, 128, 128, 256, 24, 64, 64)\n",
    "        self.inception4a = InceptionBlock(512, 112, 144, 288, 32, 64, 64)\n",
    "        self.inception4b = InceptionBlock(528, 256, 160, 320, 32, 128, 128)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.inception4c = InceptionBlock(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.inception4d = InceptionBlock(832, 384, 192, 384, 48, 128, 128)\n",
    "        \n",
    "        # Final layers\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=7, stride=1)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc = nn.Linear(1024, embedding_size)\n",
    "        self.bn2 = nn.BatchNorm(embedding_size)  # Fixed: Changed from BatchNorm1d to BatchNorm\n",
    "        \n",
    "        # L2 normalization\n",
    "        self.l2_norm = lambda x: x / mx.sqrt(mx.sum(mx.square(x), axis=1, keepdims=True))\n",
    "    \n",
    "    def __call__(self, x, training: bool = False):\n",
    "        # Initial convolutional layers\n",
    "        x = self.maxpool1(self.relu(self.bn1(self.conv1(x))))\n",
    "        \n",
    "        # Inception blocks\n",
    "        x = self.inception1(x)\n",
    "        x = self.inception2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        x = self.inception3a(x)\n",
    "        x = self.inception3b(x)\n",
    "        x = self.inception3c(x)\n",
    "        x = self.inception4a(x)\n",
    "        x = self.inception4b(x)\n",
    "        x = self.maxpool3(x)\n",
    "        \n",
    "        x = self.inception4c(x)\n",
    "        x = self.inception4d(x)\n",
    "        \n",
    "        # Final layers\n",
    "        x = self.avgpool(x)\n",
    "        x = mx.reshape(x, (-1, 1024))\n",
    "        if training:\n",
    "            x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        # L2 normalization to create the embedding\n",
    "        embedding = self.l2_norm(x)\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"Triplet loss with hard positive/negative mining\"\"\"\n",
    "    \n",
    "    def __init__(self, margin: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def __call__(self, embeddings, labels):\n",
    "        # Get the pairwise distance matrix\n",
    "        pairwise_dist = self._pairwise_distances(embeddings)\n",
    "        \n",
    "        # For each anchor, get the hardest positive\n",
    "        # First, get a mask for valid positive pairs (same class)\n",
    "        labels = mx.array(labels)\n",
    "        mask_positives = mx.equal(mx.expand_dims(labels, axis=0), mx.expand_dims(labels, axis=1))\n",
    "        \n",
    "        # Exclude the diagonal from mask_positives (distance to self is 0, not a useful positive)\n",
    "        mask_positives = mask_positives & (1 - mx.eye(labels.shape[0], dtype=mx.bool_))\n",
    "        \n",
    "        # Get hardest positives (maximum distance to positive samples)\n",
    "        hardest_positive_dist = mx.max(pairwise_dist * mask_positives.astype(mx.float32), axis=1)\n",
    "        \n",
    "        # For each anchor, get the hardest negative\n",
    "        # First, get a mask for valid negative pairs (different class)\n",
    "        mask_negatives = ~mask_positives\n",
    "        \n",
    "        # Make invalid negatives have large distance so they won't be selected\n",
    "        max_dist = mx.max(pairwise_dist)\n",
    "        neg_dist = pairwise_dist * mask_negatives.astype(mx.float32) + max_dist * (~mask_negatives).astype(mx.float32)\n",
    "        \n",
    "        # Get hardest negatives (minimum distance to negative samples)\n",
    "        hardest_negative_dist = mx.min(neg_dist, axis=1)\n",
    "        \n",
    "        # Calculate triplet loss\n",
    "        triplet_loss = mx.maximum(hardest_positive_dist - hardest_negative_dist + self.margin, 0.0)\n",
    "        \n",
    "        # Get final mean triplet loss\n",
    "        return mx.mean(triplet_loss)\n",
    "    \n",
    "    def _pairwise_distances(self, embeddings):\n",
    "        \"\"\"Compute the 2D matrix of distances between all embeddings.\"\"\"\n",
    "        # Get dot product (batch_size, batch_size)\n",
    "        dot_product = mx.matmul(embeddings, mx.transpose(embeddings))\n",
    "        \n",
    "        # Get squared L2 norm for each embedding (batch_size, 1)\n",
    "        square_norm = mx.sum(mx.square(embeddings), axis=1)\n",
    "        \n",
    "        # Calculate pairwise distance matrix \n",
    "        # ||a - b||^2 = ||a||^2 + ||b||^2 - 2 * a.dot(b)\n",
    "        distances = mx.expand_dims(square_norm, 1) + mx.expand_dims(square_norm, 0) - 2.0 * dot_product\n",
    "        \n",
    "        # Because of computation errors, some distances might be negative\n",
    "        distances = mx.maximum(distances, 0.0)\n",
    "        \n",
    "        # If we want the actual distance, we can use sqrt\n",
    "        return mx.sqrt(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path: str, target_size: Tuple[int, int] = (160, 160)) -> mx.array:\n",
    "    \"\"\"\n",
    "    Load and preprocess an image for FaceNet.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image file\n",
    "        target_size: Target size for the image (height, width)\n",
    "        \n",
    "    Returns:\n",
    "        Preprocessed image as MLX array\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import cv2\n",
    "        # Load image\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Unable to load image from {image_path}\")\n",
    "        \n",
    "        # Convert BGR to RGB\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Resize to target size\n",
    "        img = cv2.resize(img, target_size[::-1])  # CV2 takes (width, height)\n",
    "        \n",
    "        # Normalize pixel values to [-1, 1]\n",
    "        img = img.astype(np.float32) / 127.5 - 1.0\n",
    "        \n",
    "        # Convert to MLX array and add batch dimension\n",
    "        return mx.array(img).transpose(2, 0, 1)  # Convert to channels-first format\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error preprocessing image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Visualize preprocessed images\n",
    "def visualize_preprocessed_image(image_path: str):\n",
    "    \"\"\"Visualize both the original and preprocessed image\"\"\"\n",
    "    import cv2\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Load original image\n",
    "    original_img = cv2.imread(image_path)\n",
    "    original_img = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Get preprocessed image and convert back for visualization\n",
    "    preprocessed = preprocess_image(image_path)\n",
    "    # Convert from channels-first back to channels-last for display\n",
    "    preprocessed_viz = preprocessed.transpose(1, 2, 0).numpy()\n",
    "    # Rescale from [-1, 1] to [0, 1] for visualization\n",
    "    preprocessed_viz = (preprocessed_viz + 1.0) / 2.0\n",
    "    \n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax1.imshow(original_img)\n",
    "    ax1.set_title('Original Image')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    ax2.imshow(preprocessed_viz)\n",
    "    ax2.set_title('Preprocessed Image')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_weights(facenet_mlx: FaceNet, torch_model_path: str) -> FaceNet:\n",
    "    \"\"\"\n",
    "    Load pretrained weights from a PyTorch FaceNet model.\n",
    "    \n",
    "    Args:\n",
    "        facenet_mlx: MLX FaceNet model\n",
    "        torch_model_path: Path to PyTorch model file\n",
    "        \n",
    "    Returns:\n",
    "        MLX FaceNet model with loaded weights\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import torch\n",
    "        \n",
    "        # Load PyTorch model\n",
    "        torch_model = torch.load(torch_model_path, map_location=\"cpu\")\n",
    "        \n",
    "        # Create a state dict for MLX model\n",
    "        state_dict = {}\n",
    "        \n",
    "        # Map PyTorch parameter names to MLX parameter names\n",
    "        # This is a simplified example - you would need to adapt based on your specific models\n",
    "        for name, param in torch_model.items():\n",
    "            # Convert PyTorch tensor to numpy array\n",
    "            param_np = param.numpy()\n",
    "            \n",
    "            # Handle convolution weight format differences (PyTorch uses OIHW, MLX uses OIHW)\n",
    "            if len(param_np.shape) == 4:\n",
    "                # No need to transpose for this case, format is the same\n",
    "                pass\n",
    "            \n",
    "            # Add to MLX state dict - you might need to adjust the naming convention here\n",
    "            # based on your specific PyTorch model\n",
    "            state_dict[name] = mx.array(param_np)\n",
    "        \n",
    "        # Load state dict into MLX model\n",
    "        facenet_mlx.load_weights(state_dict)\n",
    "        \n",
    "        print(\"Loaded pretrained weights from PyTorch model\")\n",
    "        return facenet_mlx\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading pretrained weights: {e}\")\n",
    "        return facenet_mlx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_faces(\n",
    "    model: FaceNet,\n",
    "    unknown_face_path: str,\n",
    "    reference_faces: List[Tuple[str, str]],\n",
    "    threshold: float = 0.7\n",
    ") -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Recognize faces by comparing an unknown face against reference faces.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained FaceNet model\n",
    "        unknown_face_path: Path to unknown face image\n",
    "        reference_faces: List of (person_name, face_image_path) tuples\n",
    "        threshold: Similarity threshold (cosine similarity)\n",
    "        \n",
    "    Returns:\n",
    "        List of (person_name, similarity) tuples for matches above threshold\n",
    "    \"\"\"\n",
    "    # Preprocess unknown face\n",
    "    unknown_face = preprocess_image(unknown_face_path)\n",
    "    if unknown_face is None:\n",
    "        return []\n",
    "    \n",
    "    # Get embedding for unknown face\n",
    "    unknown_embedding = model(mx.expand_dims(unknown_face, 0))[0]\n",
    "    \n",
    "    matches = []\n",
    "    \n",
    "    # Compare with reference faces\n",
    "    for name, face_path in reference_faces:\n",
    "        # Preprocess reference face\n",
    "        ref_face = preprocess_image(face_path)\n",
    "        if ref_face is None:\n",
    "            continue\n",
    "        \n",
    "        # Get embedding for reference face\n",
    "        ref_embedding = model(mx.expand_dims(ref_face, 0))[0]\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity = mx.sum(unknown_embedding * ref_embedding) / (\n",
    "            mx.sqrt(mx.sum(unknown_embedding ** 2)) * mx.sqrt(mx.sum(ref_embedding ** 2))\n",
    "        )\n",
    "        \n",
    "        # Check if similarity is above threshold\n",
    "        similarity_value = similarity.item()\n",
    "        if similarity_value > threshold:\n",
    "            matches.append((name, similarity_value))\n",
    "    \n",
    "    # Sort matches by similarity (highest first)\n",
    "    return sorted(matches, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Visualize face recognition results\n",
    "def visualize_face_matches(\n",
    "    unknown_face_path: str,\n",
    "    matches: List[Tuple[str, float]],\n",
    "    reference_faces: List[Tuple[str, str]]\n",
    "):\n",
    "    \"\"\"Visualize the unknown face and its matches\"\"\"\n",
    "    import cv2\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Get paths of matching faces\n",
    "    matching_faces = []\n",
    "    for name, similarity in matches:\n",
    "        for ref_name, ref_path in reference_faces:\n",
    "            if name == ref_name:\n",
    "                matching_faces.append((ref_path, name, similarity))\n",
    "                break\n",
    "    \n",
    "    # Number of faces to display (unknown + matches)\n",
    "    n_faces = 1 + len(matching_faces)\n",
    "    \n",
    "    fig = plt.figure(figsize=(4 * n_faces, 5))\n",
    "    \n",
    "    # Show unknown face\n",
    "    unknown_img = cv2.imread(unknown_face_path)\n",
    "    unknown_img = cv2.cvtColor(unknown_img, cv2.COLOR_BGR2RGB)\n",
    "    ax = fig.add_subplot(1, n_faces, 1)\n",
    "    ax.imshow(unknown_img)\n",
    "    ax.set_title(\"Unknown Face\")\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Show matching faces\n",
    "    for i, (face_path, name, similarity) in enumerate(matching_faces):\n",
    "        img = cv2.imread(face_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        ax = fig.add_subplot(1, n_faces, i + 2)\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f\"{name}\\nSimilarity: {similarity:.4f}\")\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_facenet(\n",
    "    model: FaceNet,\n",
    "    train_data_path: str,\n",
    "    val_data_path: Optional[str] = None,\n",
    "    epochs: int = 10,\n",
    "    batch_size: int = 32,\n",
    "    learning_rate: float = 0.001,\n",
    "    margin: float = 0.2\n",
    "):\n",
    "    \"\"\"\n",
    "    Train FaceNet model using triplet loss.\n",
    "    \n",
    "    Args:\n",
    "        model: FaceNet model\n",
    "        train_data_path: Path to training data directory\n",
    "        val_data_path: Path to validation data directory (optional)\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Batch size\n",
    "        learning_rate: Learning rate\n",
    "        margin: Margin for triplet loss\n",
    "        \n",
    "    Returns:\n",
    "        Trained model\n",
    "    \"\"\"\n",
    "    # Set up optimizer\n",
    "    optimizer = optim.Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    # Set up loss function\n",
    "    triplet_loss = TripletLoss(margin=margin)\n",
    "    \n",
    "    # Define training step\n",
    "    @mx.compile\n",
    "    def train_step(model, images, labels):\n",
    "        def loss_fn(model):\n",
    "            embeddings = model(images, training=True)\n",
    "            loss = triplet_loss(embeddings, labels)\n",
    "            return loss\n",
    "        \n",
    "        loss, grads = nn.value_and_grad(loss_fn)(model)\n",
    "        optimizer.update(model, grads)\n",
    "        return loss\n",
    "    \n",
    "    # TODO: Implement data loading and training loop\n",
    "    # This would require implementing a DataLoader for MLX\n",
    "    # For brevity, only the core training step is shown\n",
    "    \n",
    "    print(\"Training function defined - actual implementation would require a DataLoader\")\n",
    "    return model\n",
    "\n",
    "# Simple data loader for triplet generation\n",
    "def create_triplet_batch(face_paths, person_ids, batch_size=32):\n",
    "    \"\"\"\n",
    "    Create a batch of triplets for training.\n",
    "    \n",
    "    Args:\n",
    "        face_paths: List of face image paths\n",
    "        person_ids: List of person IDs corresponding to face_paths\n",
    "        batch_size: Number of triplets in the batch\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (anchor_images, positive_images, negative_images)\n",
    "    \"\"\"\n",
    "    import random\n",
    "    \n",
    "    # Group faces by person\n",
    "    person_to_faces = {}\n",
    "    for face_path, person_id in zip(face_paths, person_ids):\n",
    "        if person_id not in person_to_faces:\n",
    "            person_to_faces[person_id] = []\n",
    "        person_to_faces[person_id].append(face_path)\n",
    "    \n",
    "    # Ensure each person has at least 2 face images\n",
    "    valid_persons = [pid for pid, faces in person_to_faces.items() if len(faces) >= 2]\n",
    "    \n",
    "    if len(valid_persons) < 2:\n",
    "        raise ValueError(\"Need at least 2 persons with 2+ images each for triplet generation\")\n",
    "    \n",
    "    anchors = []\n",
    "    positives = []\n",
    "    negatives = []\n",
    "    \n",
    "    for _ in range(batch_size):\n",
    "        # Select anchor person\n",
    "        anchor_person = random.choice(valid_persons)\n",
    "        \n",
    "        # Select anchor and positive (different images of same person)\n",
    "        anchor_face, positive_face = random.sample(person_to_faces[anchor_person], 2)\n",
    "        \n",
    "        # Select negative person (different from anchor)\n",
    "        negative_persons = [p for p in valid_persons if p != anchor_person]\n",
    "        negative_person = random.choice(negative_persons)\n",
    "        \n",
    "        # Select negative face\n",
    "        negative_face = random.choice(person_to_faces[negative_person])\n",
    "        \n",
    "        # Add to batch\n",
    "        anchors.append(preprocess_image(anchor_face))\n",
    "        positives.append(preprocess_image(positive_face))\n",
    "        negatives.append(preprocess_image(negative_face))\n",
    "    \n",
    "    # Stack images into batches\n",
    "    anchor_batch = mx.stack(anchors)\n",
    "    positive_batch = mx.stack(positives)\n",
    "    negative_batch = mx.stack(negatives)\n",
    "    \n",
    "    return anchor_batch, positive_batch, negative_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FaceNet model successfully initialized\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     12\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_count\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m parameters\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTotal parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_params\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mprint_model_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfacenet\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mprint_model_summary\u001b[39m\u001b[34m(model)\u001b[39m\n\u001b[32m      8\u001b[39m total_params = \u001b[32m0\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model.parameters().items():\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     param_count = np.prod(\u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m)\n\u001b[32m     11\u001b[39m     total_params += param_count\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_count\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m parameters\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# Initialize FaceNet model\n",
    "facenet = FaceNet(embedding_size=128)\n",
    "print(\"FaceNet model successfully initialized\")\n",
    "\n",
    "# Model summary\n",
    "def print_model_summary(model):\n",
    "    \"\"\"Print a summary of the model parameters\"\"\"\n",
    "    total_params = 0\n",
    "    for name, param in model.parameters().items():\n",
    "        param_count = np.prod(param.shape)\n",
    "        total_params += param_count\n",
    "        print(f\"{name}: {param.shape}, {param_count:,} parameters\")\n",
    "    \n",
    "    print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "\n",
    "print_model_summary(facenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'facenet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m val_data_path = \u001b[33m\"\u001b[39m\u001b[33mpath/to/validation/data\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m      6\u001b[39m trained_model = train_facenet(\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     model=\u001b[43mfacenet\u001b[49m,\n\u001b[32m      8\u001b[39m     train_data_path=train_data_path,\n\u001b[32m      9\u001b[39m     val_data_path=val_data_path,\n\u001b[32m     10\u001b[39m     epochs=\u001b[32m10\u001b[39m,\n\u001b[32m     11\u001b[39m     batch_size=\u001b[32m32\u001b[39m,\n\u001b[32m     12\u001b[39m     learning_rate=\u001b[32m0.001\u001b[39m,\n\u001b[32m     13\u001b[39m     margin=\u001b[32m0.2\u001b[39m\n\u001b[32m     14\u001b[39m )\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[32m     17\u001b[39m mx.save(\u001b[33m\"\u001b[39m\u001b[33mfacenet_mlx_model.npz\u001b[39m\u001b[33m\"\u001b[39m, trained_model.parameters())\n",
      "\u001b[31mNameError\u001b[39m: name 'facenet' is not defined"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "train_data_path = \"path/to/training/data\"\n",
    "val_data_path = \"path/to/validation/data\"\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_facenet(\n",
    "    model=facenet,\n",
    "    train_data_path=train_data_path,\n",
    "    val_data_path=val_data_path,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.001,\n",
    "    margin=0.2\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "mx.save(\"facenet_mlx_model.npz\", trained_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
